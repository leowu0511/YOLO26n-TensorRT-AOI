#include "YoloDetector.h"
#include <opencv2/cudawarping.hpp>
#include <opencv2/cudaarithm.hpp>
#include <opencv2/cudaimgproc.hpp>

/**
 * å»ºæ§‹å­ï¼šåˆå§‹åŒ–æˆå“¡è®Šæ•¸ä¸¦é åˆ†é… NMS å·¥ä½œç·©è¡å€
 */
YoloDetector::YoloDetector(const std::string& enginePath) : mEnginePath(enginePath) {
    // é åˆ†é… NMS ç©ºé–“ä»¥é¿å…æ¨è«–éç¨‹ä¸­çš„è¨˜æ†¶é«”æŠ–å‹•
    mClassIds.reserve(1000);
    mConfidences.reserve(1000);
    mBoxes.reserve(1000);
    mNmsIndices.reserve(1000);
    mOutputHostBuffer.resize(50400); // é‡å°å–® batch è¼¸å‡º (1 * 6 * 8400)
}

/**
 * è§£æ§‹å­ï¼šç¢ºä¿æ‰€æœ‰ CUDA è³‡æºèˆ‡ TensorRT ç‰©ä»¶æ­£ç¢ºé‡‹æ”¾
 */
YoloDetector::~YoloDetector() {
    if (mInputBuffer) cudaFree(mInputBuffer);
    if (mOutputBuffer) cudaFree(mOutputBuffer);
    if (mPinnedOutputBuffer) cudaFreeHost(mPinnedOutputBuffer);
    if (mStream) cudaStreamDestroy(mStream);

    if (mContext) delete mContext;
    if (mEngine) delete mEngine;
    if (mRuntime) delete mRuntime;
}

/**
 * åˆå§‹åŒ–å¼•æ“ï¼šè¼‰å…¥æ¨¡å‹ã€åˆ†é…é¡¯å­˜èˆ‡é…ç½® Tensor åœ°å€
 */
bool YoloDetector::init() {
    std::ifstream file(mEnginePath, std::ios::binary);
    if (!file.good()) {
        std::cerr << "[Error] Cannot find engine file: " << mEnginePath << std::endl;
        return false;
    }

    // è®€å–æ¨¡å‹äºŒé€²ä½è³‡æ–™
    file.seekg(0, file.end);
    size_t size = file.tellg();
    file.seekg(0, file.beg);
    std::vector<char> engineData(size);
    file.read(engineData.data(), size);
    file.close();

    // å»ºç«‹ TensorRT Runtime èˆ‡ Engine
    mRuntime = nvinfer1::createInferRuntime(mLogger);
    if (!mRuntime) { std::cerr << "[Error] Failed to create Runtime"; return false; }

    mEngine = mRuntime->deserializeCudaEngine(engineData.data(), size);
    if (!mEngine) { std::cerr << "[Error] Failed to deserialize Engine"; return false; }

    mContext = mEngine->createExecutionContext();
    if (!mContext) { std::cerr << "[Error] Failed to create Context"; return false; }

    // åˆå§‹åŒ– CUDA ä¸²æµ
    if (cudaStreamCreate(&mStream) != cudaSuccess) return false;
    mCvStream = cv::cuda::StreamAccessor::wrapStream(mStream);

    // è¨ˆç®—ç·©è¡å€å¤§å°
    mInputSize = 1 * 3 * 640 * 640 * sizeof(float);
    mOutputSize = 1 * 6 * 8400 * sizeof(float);

    // åˆ†é… GPU é¡¯å­˜
    if (cudaMalloc(&mInputBuffer, mInputSize) != cudaSuccess) return false;
    if (cudaMalloc(&mOutputBuffer, mOutputSize) != cudaSuccess) return false;

    // åˆ†é… Pinned Memory ä»¥åŠ é€Ÿ PCIe æ•¸æ“šå‚³è¼¸ (H2D/D2H)
    if (cudaMallocHost(&mPinnedOutputBuffer, mOutputSize) != cudaSuccess) {
        std::cerr << "[Warning] Pinned Memory allocation failed, using standard host memory." << std::endl;
        mUsePinnedMemory = false;
    }

    // ğŸ’¡ é‡å° TensorRT 10 C++ APIï¼šç¶å®š Tensor åœ°å€
    // é›–ç„¶ Python API å·²æ›´æ–°ï¼Œä½†åœ¨ C++ ICudaEngine ä¸­ getIOTensorName ä»ç‚ºæ¨™æº–ç”¨æ³•
    mContext->setTensorAddress(mEngine->getIOTensorName(0), mInputBuffer);
    mContext->setTensorAddress(mEngine->getIOTensorName(1), mOutputBuffer);

    // é åˆ†é… GPU é è™•ç†ä¸­é–“ç·©è¡å€ï¼Œç¢ºä¿é›¶ malloc å»¶é²
    m_d_img.create(mInputH, mInputW, CV_8UC3);
    m_d_resized.create(mInputH, mInputW, CV_8UC3);
    m_d_rgb.create(mInputH, mInputW, CV_8UC3);
    m_d_float.create(mInputH, mInputW, CV_32FC3);

    // é…ç½®å½±åƒå¹³é¢ (CHW æ ¼å¼)
    m_chw_channels.clear();
    for (int i = 0; i < 3; ++i) {
        m_chw_channels.push_back(cv::cuda::GpuMat(mInputH, mInputW, CV_32FC1,
            (float*)mInputBuffer + i * mInputW * mInputH));
    }

    std::cout << "[System] YoloDetector Engine verified and ready [RTX 4060]" << std::endl;
    return true;
}

/**
 * å¯¦æˆ°æ¨¡å¼ï¼šæ¥æ”¶ CPU Mat ä¸¦åŸ·è¡Œå®Œæ•´çš„ä¸Šå‚³èˆ‡æ¨è«–æµç¨‹
 */
std::vector<Detection> YoloDetector::detect(const cv::Mat& img) {
    if (img.empty()) return {};

    // 1. ä¸Šå‚³è‡³ GPU ä¸¦åŸ·è¡Œé è™•ç†
    preprocessGPU(img);

    // 2. åŸ·è¡Œæ¨è«–
    mContext->enqueueV3(mStream);

    // 3. éåŒæ­¥æ‹·è²çµæœå› Host
    const float* dataPtr;
    if (mUsePinnedMemory) {
        cudaMemcpyAsync(mPinnedOutputBuffer, mOutputBuffer, mOutputSize, cudaMemcpyDeviceToHost, mStream);
        cudaStreamSynchronize(mStream); // å”¯ä¸€åŒæ­¥é»
        dataPtr = mPinnedOutputBuffer;
    }
    else {
        cudaMemcpyAsync(mOutputHostBuffer.data(), mOutputBuffer, mOutputSize, cudaMemcpyDeviceToHost, mStream);
        cudaStreamSynchronize(mStream);
        dataPtr = mOutputHostBuffer.data();
    }

    return postprocess(dataPtr, img.size());
}

/**
 * é›¶æ‹·è²æ¨¡å¼ï¼šç›´æ¥è™•ç†é¡¯å­˜ä¸­çš„å½±åƒ (æ¸¬è©¦ç®—åŠ›æ¥µé™)
 */
std::vector<Detection> YoloDetector::detectGpu(const cv::cuda::GpuMat& d_img) {
    if (d_img.empty()) return {};

    // 1. å…§éƒ¨ GPU é è™•ç† (é›¶ CPU åƒèˆ‡)
    preprocessGpuDirect(d_img);

    // 2. åŸ·è¡Œæ¨è«–
    mContext->enqueueV3(mStream);

    // 3. å‚³è¼¸åµæ¸¬çµæœ
    cudaMemcpyAsync(mPinnedOutputBuffer, mOutputBuffer, mOutputSize, cudaMemcpyDeviceToHost, mStream);
    cudaStreamSynchronize(mStream);

    return postprocess(mPinnedOutputBuffer, d_img.size());
}

/**
 * å°è£å½±åƒä¸Šå‚³èˆ‡è™•ç†
 */
void YoloDetector::preprocessGPU(const cv::Mat& img) {
    m_d_img.upload(img, mCvStream);
    preprocessGpuDirect(m_d_img);
}

/**
 * GPU ç›´æ¥é è™•ç†ï¼šç¸®æ”¾ã€è‰²å½©ç©ºé–“è½‰æ›èˆ‡æ­¸ä¸€åŒ– (CHW)
 */
void YoloDetector::preprocessGpuDirect(const cv::cuda::GpuMat& d_img) {
    cv::cuda::resize(d_img, m_d_resized, cv::Size(mInputW, mInputH), 0, 0, cv::INTER_LINEAR, mCvStream);
    cv::cuda::cvtColor(m_d_resized, m_d_rgb, cv::COLOR_BGR2RGB, 3, mCvStream);
    m_d_rgb.convertTo(m_d_float, CV_32FC3, 1.0 / 255.0, 0.0, mCvStream);
    cv::cuda::split(m_d_float, m_chw_channels, mCvStream);
}

/**
 * å¾Œè™•ç†ï¼šè§£æ Tensor ä¸¦åŸ·è¡Œ NMS (å·²å„ªåŒ–å…§å­˜åˆ†é…)
 */
std::vector<Detection> YoloDetector::postprocess(const float* data, const cv::Size& originalSize) {
    mClassIds.clear();
    mConfidences.clear();
    mBoxes.clear();
    mNmsIndices.clear();

    const float scale = std::min((float)mInputW / originalSize.width, (float)mInputH / originalSize.height);
    const float xOffset = (mInputW - (originalSize.width * scale)) / 2.0f;
    const float yOffset = (mInputH - (originalSize.height * scale)) / 2.0f;
    const float invScale = 1.0f / scale;

    for (int i = 0; i < 8400; ++i) {
        const float* row = data + (i * 6);
        if (row[4] > 0.25f) {
            int left = static_cast<int>((row[0] - row[2] * 0.5f - xOffset) * invScale);
            int top = static_cast<int>((row[1] - row[3] * 0.5f - yOffset) * invScale);
            int width = static_cast<int>(row[2] * invScale);
            int height = static_cast<int>(row[3] * invScale);

            // é‚Šç•Œå®‰å…¨æª¢æŸ¥
            left = std::max(0, std::min(left, originalSize.width - 1));
            top = std::max(0, std::min(top, originalSize.height - 1));
            width = std::max(1, std::min(width, originalSize.width - left));
            height = std::max(1, std::min(height, originalSize.height - top));

            mClassIds.push_back(static_cast<int>(row[5]));
            mConfidences.push_back(row[4]);
            mBoxes.push_back(cv::Rect(left, top, width, height));
        }
    }

    // åŸ·è¡Œ NMS
    cv::dnn::NMSBoxes(mBoxes, mConfidences, 0.25f, 0.45f, mNmsIndices);

    std::vector<Detection> detections;
    detections.reserve(mNmsIndices.size());
    for (int idx : mNmsIndices) {
        Detection det;
        det.classId = mClassIds[idx];
        det.confidence = mConfidences[idx];
        det.box = mBoxes[idx];
        detections.push_back(det);
    }
    return detections;
}
